# -*- coding: utf-8 -*-
"""swathi_final_assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACv5SpbxZEiIfpzZ0kwdrqONHJs6eP8t
"""

import pandas as pd

# Load the train and test datasets

train_data = pd.read_csv('train_ctrUa4K.csv')
test_data = pd.read_csv('test_lAUu6dG.csv')

from google.colab import drive
drive.mount('/content/drive')

train_data.head()

test_data.head()

train_data.info()

import numpy as np #Supports efficient mathematical operations on large, multi-dimensional arrays and matrices, ideal for scientific computations
import matplotlib.pyplot as plt #A plotting library for creating static, animated, and interactive visualizations in Python.
import seaborn as sns #Built on matplotlib, seaborn offers a high-level interface for making attractive and informative statistical graphics
from sklearn.model_selection import train_test_split #Used to split data into training and testing sets for model evaluation.
from sklearn.preprocessing import StandardScaler #Standardizes features by removing the mean and scaling to unit variance, helping improve model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score #Provides metrics like mean absolute error, mean squared error, and R² score for evaluating model performance.
from tensorflow.keras.models import Sequential #Sequential is a linear stack of neural network layers, facilitating model creation in Keras
from tensorflow.keras.layers import Dense, Dropout

test_data.info()

test_data.shape

train_data.shape

test_data.dtypes

train_data.dtypes

# Define columns by type for easy handling
categorical_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']
numerical_cols = ['LoanAmount', 'Loan_Amount_Term']

# Fill missing values in train dataset
for col in categorical_cols:
    train_data[col].fillna(train_data[col].mode()[0], inplace=True)

for col in numerical_cols:
    train_data[col].fillna(train_data[col].median(), inplace=True)

# Fill missing values in test dataset
for col in categorical_cols:
    test_data[col].fillna(test_data[col].mode()[0], inplace=True)

for col in numerical_cols:
    test_data[col].fillna(test_data[col].median(), inplace=True)

# Perform one-hot encoding for categorical variables, dropping the first level to avoid multicollinearity
train_data = pd.get_dummies(train_data, columns=['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'], drop_first=True)
test_data = pd.get_dummies(test_data, columns=['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'], drop_first=True)

# Create a new feature for total income in both train and test datasets
train_data['TotalIncome'] = train_data['ApplicantIncome'] + train_data['CoapplicantIncome']
test_data['TotalIncome'] = test_data['ApplicantIncome'] + test_data['CoapplicantIncome']

# Drop original income columns if not needed
train_data.drop(['ApplicantIncome', 'CoapplicantIncome'], axis=1, inplace=True)
test_data.drop(['ApplicantIncome', 'CoapplicantIncome'], axis=1, inplace=True)

#scaling numerical features
# List of numerical columns to scale
numerical_features = ['LoanAmount', 'Loan_Amount_Term', 'TotalIncome']

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the train data
train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])

# Only transform the test data
test_data[numerical_features] = scaler.transform(test_data[numerical_features])

# Verify no missing values remain
print(train_data.isnull().sum().sum())  # Should print 0
print(test_data.isnull().sum().sum())   # Should print 0

# Check for column consistency between train and test datasets
print(train_data.columns)
print(test_data.columns)

#modelling

# Separate features and target variable
X = train_data.drop(columns=['Loan_ID', 'Loan_Status'])
y = train_data['Loan_Status'].apply(lambda x: 1 if x == 'Y' else 0)  # Convert target to 1 (Yes) and 0 (No)

# Split into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

#Let’s train several models and evaluate their accuracy.
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    print(f"{name} Accuracy: {accuracy:.4f}")

from sklearn.ensemble import VotingClassifier

# Ensemble using VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('lr', LogisticRegression(max_iter=1000)),
                ('rf', RandomForestClassifier(random_state=42)),
                ('gb', GradientBoostingClassifier(random_state=42))],
    voting='hard'
)

voting_clf.fit(X_train, y_train)
voting_accuracy = voting_clf.score(X_val, y_val)
print("Voting Classifier Accuracy:", voting_accuracy)

from sklearn.model_selection import GridSearchCV

# Example for Random Forest hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

#select best model

best_model = RandomForestClassifier(random_state=42)
best_model.fit(X, y)

# Prepare test data by dropping Loan_ID (but keep it for the final submission file)
X_test = test_data.drop(columns=['Loan_ID'])
test_pred = best_model.predict(X_test)
test_pred = ['Y' if pred == 1 else 'N' for pred in test_pred]

# Create the submission DataFrame
submission = pd.DataFrame({
    'Loan_ID': test_data['Loan_ID'],
    'Loan_Status': test_pred
})

# Save the submission file
submission.to_csv('loan_eligibility_submission.csv', index=False)
print("Submission file created successfully!")

